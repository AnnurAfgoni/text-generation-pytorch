{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary package\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open text file\n",
    "with open(\"laskar_pelangi.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seorang bapak tua berwajah sabar, Bapak K.A. Harfan Efendy Noor, \\nsang kepala sekolah dan seorang wa'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first 100 characters\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "\n",
    "# pair int with character in text\n",
    "int2char = {i: ch for i, ch in enumerate(set(text))}\n",
    "\n",
    "# reverse\n",
    "char2int = {ch: i for (i, ch) in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40, 33, 81, 62, 47, 26, 30, 21, 90, 47])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get text encoding result\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692185"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \"\"\"\n",
    "    function to apply one hot encoding in sequence array\n",
    "\n",
    "    params\n",
    "        arr: sequence array\n",
    "        n_labels: number of characters (size of arr)\n",
    "\n",
    "    returns\n",
    "        one hot encoded array\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.array([[3, 5, 2]])\n",
    "\n",
    "one_hot_encode(sample, n_labels=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "\n",
    "    batch_size_total = batch_size * seq_length\n",
    "\n",
    "    n_batches = len(arr) // batch_size_total\n",
    "\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "\n",
    "        x = arr[:, n:n+seq_length]\n",
    "\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        \n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        # create char dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        # define layers of the model\n",
    "        self.lstm = nn.LSTM(\n",
    "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
    "        )\n",
    "\n",
    "        # define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        # define the final, fully connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        # get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "\n",
    "        # stack up LSTM using view\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "\n",
    "        # put x through the fully connectedd layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        # create two new tensors with sizes n_layers x batch_size x n_hidden\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda()\n",
    "            )\n",
    "        else:\n",
    "            hidden = (\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_()\n",
    "            )\n",
    "\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # create training and validation\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "\n",
    "            # one hot\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # create new variables for the hidden state\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "\n",
    "                # get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "\n",
    "                    # one hot encode\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    inputs, targets = x, y\n",
    "                    if (train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                # reset to train mode after iterate validation\n",
    "                net.train()\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch: {e+1}/{epochs}...\",\n",
    "                    f\"Step: {counter}...\",\n",
    "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses))\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(93, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=93, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.1452... Val Loss: 3.0748\n",
      "Epoch: 1/20... Step: 20... Loss: 3.0805... Val Loss: 3.0363\n",
      "Epoch: 1/20... Step: 30... Loss: 3.0465... Val Loss: 3.0299\n",
      "Epoch: 1/20... Step: 40... Loss: 3.0371... Val Loss: 3.0294\n",
      "Epoch: 2/20... Step: 50... Loss: 3.0363... Val Loss: 3.0252\n",
      "Epoch: 2/20... Step: 60... Loss: 3.0531... Val Loss: 3.0230\n",
      "Epoch: 2/20... Step: 70... Loss: 3.0357... Val Loss: 3.0190\n",
      "Epoch: 2/20... Step: 80... Loss: 3.0309... Val Loss: 3.0089\n",
      "Epoch: 2/20... Step: 90... Loss: 3.0025... Val Loss: 2.9837\n",
      "Epoch: 3/20... Step: 100... Loss: 2.9395... Val Loss: 2.9099\n",
      "Epoch: 3/20... Step: 110... Loss: 2.8034... Val Loss: 2.7781\n",
      "Epoch: 3/20... Step: 120... Loss: 2.6861... Val Loss: 2.6372\n",
      "Epoch: 3/20... Step: 130... Loss: 2.5561... Val Loss: 2.5351\n",
      "Epoch: 3/20... Step: 140... Loss: 2.4982... Val Loss: 2.4621\n",
      "Epoch: 4/20... Step: 150... Loss: 2.4405... Val Loss: 2.3798\n",
      "Epoch: 4/20... Step: 160... Loss: 2.3731... Val Loss: 2.3407\n",
      "Epoch: 4/20... Step: 170... Loss: 2.3584... Val Loss: 2.3088\n",
      "Epoch: 4/20... Step: 180... Loss: 2.2967... Val Loss: 2.2979\n",
      "Epoch: 4/20... Step: 190... Loss: 2.3218... Val Loss: 2.2693\n",
      "Epoch: 5/20... Step: 200... Loss: 2.2885... Val Loss: 2.2423\n",
      "Epoch: 5/20... Step: 210... Loss: 2.2607... Val Loss: 2.2309\n",
      "Epoch: 5/20... Step: 220... Loss: 2.2222... Val Loss: 2.2032\n",
      "Epoch: 5/20... Step: 230... Loss: 2.2307... Val Loss: 2.1919\n",
      "Epoch: 5/20... Step: 240... Loss: 2.2196... Val Loss: 2.1798\n",
      "Epoch: 6/20... Step: 250... Loss: 2.1937... Val Loss: 2.1643\n",
      "Epoch: 6/20... Step: 260... Loss: 2.1898... Val Loss: 2.1480\n",
      "Epoch: 6/20... Step: 270... Loss: 2.1659... Val Loss: 2.1357\n",
      "Epoch: 6/20... Step: 280... Loss: 2.1592... Val Loss: 2.1245\n",
      "Epoch: 7/20... Step: 290... Loss: 2.1544... Val Loss: 2.1146\n",
      "Epoch: 7/20... Step: 300... Loss: 2.1534... Val Loss: 2.1029\n",
      "Epoch: 7/20... Step: 310... Loss: 2.1529... Val Loss: 2.0931\n",
      "Epoch: 7/20... Step: 320... Loss: 2.1268... Val Loss: 2.0890\n",
      "Epoch: 7/20... Step: 330... Loss: 2.1177... Val Loss: 2.0751\n",
      "Epoch: 8/20... Step: 340... Loss: 2.0870... Val Loss: 2.0665\n",
      "Epoch: 8/20... Step: 350... Loss: 2.0950... Val Loss: 2.0559\n",
      "Epoch: 8/20... Step: 360... Loss: 2.0972... Val Loss: 2.0454\n",
      "Epoch: 8/20... Step: 370... Loss: 2.0441... Val Loss: 2.0374\n",
      "Epoch: 8/20... Step: 380... Loss: 2.0663... Val Loss: 2.0240\n",
      "Epoch: 9/20... Step: 390... Loss: 2.0451... Val Loss: 2.0144\n",
      "Epoch: 9/20... Step: 400... Loss: 2.0513... Val Loss: 2.0066\n",
      "Epoch: 9/20... Step: 410... Loss: 2.0377... Val Loss: 1.9982\n",
      "Epoch: 9/20... Step: 420... Loss: 2.0022... Val Loss: 1.9866\n",
      "Epoch: 9/20... Step: 430... Loss: 2.0226... Val Loss: 1.9768\n",
      "Epoch: 10/20... Step: 440... Loss: 2.0167... Val Loss: 1.9708\n",
      "Epoch: 10/20... Step: 450... Loss: 2.0075... Val Loss: 1.9587\n",
      "Epoch: 10/20... Step: 460... Loss: 1.9760... Val Loss: 1.9511\n",
      "Epoch: 10/20... Step: 470... Loss: 1.9979... Val Loss: 1.9392\n",
      "Epoch: 10/20... Step: 480... Loss: 1.9915... Val Loss: 1.9305\n",
      "Epoch: 11/20... Step: 490... Loss: 1.9741... Val Loss: 1.9237\n",
      "Epoch: 11/20... Step: 500... Loss: 1.9655... Val Loss: 1.9167\n",
      "Epoch: 11/20... Step: 510... Loss: 1.9632... Val Loss: 1.9071\n",
      "Epoch: 11/20... Step: 520... Loss: 1.9436... Val Loss: 1.8980\n",
      "Epoch: 12/20... Step: 530... Loss: 1.9356... Val Loss: 1.8914\n",
      "Epoch: 12/20... Step: 540... Loss: 1.9492... Val Loss: 1.8843\n",
      "Epoch: 12/20... Step: 550... Loss: 1.9409... Val Loss: 1.8775\n",
      "Epoch: 12/20... Step: 560... Loss: 1.9013... Val Loss: 1.8663\n",
      "Epoch: 12/20... Step: 570... Loss: 1.8989... Val Loss: 1.8627\n",
      "Epoch: 13/20... Step: 580... Loss: 1.8872... Val Loss: 1.8508\n",
      "Epoch: 13/20... Step: 590... Loss: 1.8816... Val Loss: 1.8453\n",
      "Epoch: 13/20... Step: 600... Loss: 1.8913... Val Loss: 1.8372\n",
      "Epoch: 13/20... Step: 610... Loss: 1.8407... Val Loss: 1.8311\n",
      "Epoch: 13/20... Step: 620... Loss: 1.8710... Val Loss: 1.8242\n",
      "Epoch: 14/20... Step: 630... Loss: 1.8515... Val Loss: 1.8178\n",
      "Epoch: 14/20... Step: 640... Loss: 1.8674... Val Loss: 1.8124\n",
      "Epoch: 14/20... Step: 650... Loss: 1.8567... Val Loss: 1.8032\n",
      "Epoch: 14/20... Step: 660... Loss: 1.8210... Val Loss: 1.7963\n",
      "Epoch: 14/20... Step: 670... Loss: 1.8398... Val Loss: 1.7907\n",
      "Epoch: 15/20... Step: 680... Loss: 1.8398... Val Loss: 1.7838\n",
      "Epoch: 15/20... Step: 690... Loss: 1.8349... Val Loss: 1.7782\n",
      "Epoch: 15/20... Step: 700... Loss: 1.8142... Val Loss: 1.7727\n",
      "Epoch: 15/20... Step: 710... Loss: 1.8290... Val Loss: 1.7661\n",
      "Epoch: 15/20... Step: 720... Loss: 1.8365... Val Loss: 1.7585\n",
      "Epoch: 16/20... Step: 730... Loss: 1.8124... Val Loss: 1.7522\n",
      "Epoch: 16/20... Step: 740... Loss: 1.8140... Val Loss: 1.7470\n",
      "Epoch: 16/20... Step: 750... Loss: 1.7882... Val Loss: 1.7431\n",
      "Epoch: 16/20... Step: 760... Loss: 1.7809... Val Loss: 1.7367\n",
      "Epoch: 17/20... Step: 770... Loss: 1.7806... Val Loss: 1.7304\n",
      "Epoch: 17/20... Step: 780... Loss: 1.7975... Val Loss: 1.7284\n",
      "Epoch: 17/20... Step: 790... Loss: 1.7895... Val Loss: 1.7208\n",
      "Epoch: 17/20... Step: 800... Loss: 1.7480... Val Loss: 1.7157\n",
      "Epoch: 17/20... Step: 810... Loss: 1.7411... Val Loss: 1.7108\n",
      "Epoch: 18/20... Step: 820... Loss: 1.7479... Val Loss: 1.7059\n",
      "Epoch: 18/20... Step: 830... Loss: 1.7421... Val Loss: 1.7017\n",
      "Epoch: 18/20... Step: 840... Loss: 1.7623... Val Loss: 1.6969\n",
      "Epoch: 18/20... Step: 850... Loss: 1.6971... Val Loss: 1.6928\n",
      "Epoch: 18/20... Step: 860... Loss: 1.7285... Val Loss: 1.6861\n",
      "Epoch: 19/20... Step: 870... Loss: 1.7194... Val Loss: 1.6816\n",
      "Epoch: 19/20... Step: 880... Loss: 1.7422... Val Loss: 1.6765\n",
      "Epoch: 19/20... Step: 890... Loss: 1.7271... Val Loss: 1.6739\n",
      "Epoch: 19/20... Step: 900... Loss: 1.6914... Val Loss: 1.6689\n",
      "Epoch: 19/20... Step: 910... Loss: 1.7084... Val Loss: 1.6634\n",
      "Epoch: 20/20... Step: 920... Loss: 1.7174... Val Loss: 1.6597\n",
      "Epoch: 20/20... Step: 930... Loss: 1.7117... Val Loss: 1.6563\n",
      "Epoch: 20/20... Step: 940... Loss: 1.6980... Val Loss: 1.6529\n",
      "Epoch: 20/20... Step: 950... Loss: 1.7097... Val Loss: 1.6462\n",
      "Epoch: 20/20... Step: 960... Loss: 1.7248... Val Loss: 1.6417\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "\n",
    "# train\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rnn_20_epoch.net\"\n",
    "\n",
    "checkpoint = {\n",
    "    \"n_hidden\": net.n_hidden,\n",
    "    \"n_layers\": net.n_layers,\n",
    "    \"state_dict\": net.state_dict(),\n",
    "    \"tokens\": net.chars\n",
    "}\n",
    "\n",
    "with open(model_name, \"wb\") as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "\n",
    "    # tensor input\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the character probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    if(train_on_gpu):\n",
    "        p = p.cpu() # move to cpu\n",
    "    \n",
    "    # get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "    # select the likely next character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime=\"The\", top_k=None):\n",
    "\n",
    "    if (train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suatu hari dari sampul tampa bertungguh \n",
      "manakiana melomoki duru sisi tika sangat dera mereka diamasiku dinggetar. \n",
      "\n",
      "\"Bu Maram \n",
      "dan. \n",
      "\n",
      "Sekaling tak pelungan kami depat pelungi kanasatan mulam \n",
      "kami tidah di \n",
      "belikung,, berdangat ketadakun \n",
      "pari keritas kemalika ika tak bersenyundang penggamput pisuk kulut kami sama. Seperanya buah karena koreka adelah persus mata buah semua tak punya sepanjang. Aku tahun sebaah mengimiranku dalam sumbah daun saju selumah di sapang dari berakang. Sebenah poriati sita. \n",
      "\n",
      "Kama pulin tersekolasi. \n",
      "\n",
      "Ketakian, beraksi du pada duar kembatari di buahan banya karena sekali melahat bangku delan bentak-berut sama terbosanga dan sebuah kami mendekatkan poson kurang-tambang bukan, tapi bersisut komanya \n",
      "dengan birukan sembarang, sekorah sepeda \n",
      "meruasakan dan sungah terpulainya. Kiti keterasa karena sendiri. \n",
      "\n",
      "Aka maka mempuka merihatikan sepinya senduri di soko para siata bertaru tak berbuang duan tiasa ke mendapat \n",
      "pesan belun binga me- \n",
      "ngurtakkan kati seperti pelahar dan \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime=\"suatu hari\", top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"rnn_20_epoch.net\", \"rb\") as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "loaded = CharRNN(\n",
    "    checkpoint[\"tokens\"],\n",
    "    n_hidden=checkpoint[\"n_hidden\"],\n",
    "    n_layers=checkpoint[\"n_layers\"] \n",
    ")\n",
    "\n",
    "loaded.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seorang bapakan simasinya di diatang berak dirindah sumpah \n",
      "tapi tubanya. \n",
      "\n",
      "Sebuah kelak minit mereka. Suaru duler bahan, selelah masik ketika tembah, bentak kepantik \n",
      "meroka sumbai kami sepatai ke atah sekarang-bangang dengan seperti pusar manat \n",
      "menyilah semuanya belapan komisi sunda dua membuat di sukunan di saperah. Makanya semua sebugang berusas kolong korek sampai keladu pohon kemanisan menghomponkong berkorik, bahatan beratuah \n",
      "kami. Suaranya, selerang kanyangan, \n",
      "berkemadikan kumping sebertik pesangkan \n",
      "masam duru ke buahan, tangan kuning busan sina menyelahkan semengari satik menjelat di berunggah punya di dalam kemusia dan belapanan du semadah pertakangan \n",
      "karang \n",
      "karang di sana mengantar burung kilamat menyentuh, kute-diatasi seberapa sebuah tunati sepada \n",
      "di sini sekulih membawa sebuah. Mahar \n",
      "satu. \n",
      "\n",
      "\n",
      "Kami menjudukkan. \n",
      "\n",
      "\n",
      "\n",
      "188 \n",
      "\n",
      "\n",
      "\n",
      "Andrea Hiratakan musih kami tampak pengintang sudah dalam masih. \n",
      "\n",
      "Aku bahas \n",
      "kati. \n",
      "\n",
      "\n",
      "\n",
      "49 \n",
      "\n",
      "\n",
      "\n",
      "Andrea Hirata \n",
      "\n",
      "terahan semakin tangan-barang \n",
      "mangkut menyarak- \n",
      "ni sampal sekali di semengir semadang mencendak dengan bulu sepangga tampak belasaratan. \n",
      "\n",
      "Kemukian berkuat saka tapu mencira masih sekolah dinggat berada dua akulah di dengan, beradu di daranya seperti \n",
      "sang dari dia muluhkan karena kerenangan di anas mengumpakkan kepulihnya tak bersama \n",
      "biasa. Kami dianggar kati, saja.\". \n",
      "\n",
      "Kelakatan menampar di kata keluangan pun membilangan kanyu di sola masi terbusi seperti pun mukang dengan kami mereka teranggat masuk sekolah mengambut barung, masih semua mengenangkan sekali berbadar, \n",
      "selama memindang \n",
      "kala meresa dalam pontan kareng mendari \n",
      "bersamunga dua bulu torot melambat \n",
      "dengan dalam kingkung dan berserempat kinas menyerakkan beratas kami sada \n",
      "merakat kamin di suara mendengal masuk korut sekorat \n",
      "melihat berana \n",
      "\n",
      "\n",
      "\n",
      "181 \n",
      "\n",
      "\n",
      "\n",
      "Andraa Hirata \n",
      "\n",
      "\n",
      "\n",
      "Kasis sekolah \n",
      "dan tak berduri di dan sambil, temput sami sebagai \n",
      "buki di menggaranki \n",
      "kantang \n",
      "seperti pantang daliman berserema diamas dan \n",
      "merema mereka tertahi kami dan tidak berkulang buah bah\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, top_k=5, prime=\"seorang bapak\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
